---
title: "CanadianRetailSales"
output: html_document
---

In this document we develop a simple forecasting model of Canadian Retail Sales.
The first thing I done was to look at the data you provided. It describes unadjusted and seasonally adjusted Monthly Retail Trades in Canada from **January 2000** to **July 2015**.

Looking at the date the data were accessed the 20th of October 2015. Looking at the Statistics Canada website, I first noticed that the data were last modified the 22nd of October 2015 and that the numbers were slightly different.

Definition: Retail trade is defined in the International Standard Industrial Classification (ISIC) as the re-sale (sale without transformation) of new and used goods to the general public, for personal or household consumption or utilisation.

By researching the [Statistics Canada help page](http://www23.statcan.gc.ca/imdb/p2SV.pl?Function=getSurvey&SDDS=2406), we note that that seasonally adjusted data is built using the X-12-ARIMA method.

```{r, echo=FALSE, message=FALSE}
library(plyr)
library(dplyr)
library(xts)
library(zoo)
library(ggplot2)
library(forecast)
library(tseries)
```
## Preliminaries:

### Formatting the data:

First the data provided is not ideal for an analysis using R so I had to reformat it.

```{r}
base.data <- read.csv('./data/cansim__Jan-2000_Jul-2015.csv', header=FALSE)

data <-
  base.data %>%
  select(-V1, -V2, -V3) %>% # Remove the description columns
  slice(5:7) %>%            # Only select row containing data
  t %>% data.frame %>%      # Transpose and cast to data.frame
  rename(adjustments=X1) %>%
  rename(unadjusted=X2) %>%
  rename(seasonally.adjusted=X3) %>%
  mutate(seasonally.adjusted=as.numeric(as.character(seasonally.adjusted))) %>%
  mutate(unadjusted=as.numeric(as.character(unadjusted))) %>%
  mutate(adjustments=as.Date(
                        as.character(x=paste("01-", adjustments, sep="")),
                        format="%d-%b-%Y")
         )# Converts the adjustments. You need to paste a day to be able to convert such format to a date.
```



Now it will be easier to work with such format.

### First look at the data:

```{r}
g.data.with.regression <-
  ggplot(data, aes(x=adjustments, y=unadjusted, color='unadjusted')) +
  geom_smooth(method='lm', color='black') + geom_line() +
  geom_line(data=data, aes(x=adjustments, y=seasonally.adjusted, color='seasonally.adjusted')) +
  xlab('Time') + ylab('Retail Sales')

g.data.with.regression
```

From here, we can note a few observations:

* The retail sales are strongly seasonal.
* The time plot shows a sudden change, particulary in 2009.
* The variability of the seasonal components seems constant.


We want to build a model that can forecast unadjusted retail sales data, let's create timeseries objects for
that data.

```{r}
unadjusted.ts <- ts(data$unadjusted, frequency=12)
```

### Stationarity:
From those observations, we can suppose that our time series is not stationary process.
But let's test that assumption using the **Augmented Dickey-Fuller Test**, the **Box test** and the **KPSS test**.


```{r  message=FALSE, warning=FALSE}
results.unadjusted <- list(
  adf=adf.test(unadjusted.ts)$p.value < 0.05,
  box=Box.test(unadjusted.ts, lag=12, type='Ljung-Box')$p.value < 0.05,
  kpss=kpss.test(unadjusted.ts)$p.value > 0.05
)
results.unadjusted
```

From those test, we note that the three tests don't give the same results of stationarity.
Moreover, if we have a look at the Autocorrelation function (ACF) and the Partial autocorrelation function (PACF) of our data. The ACF shows large autocorrelations that diminish very slowly at large lags. This is usually the
signature of a non-stationary time series.

```{r}
tsdisplay(unadjusted.ts)
```

Thus, we can safely assume the data is not stationary which means we need to transform the data such that 
our process is stationary on mean and variance.

Since non stationary a good idea would be to model the data using an ARIMA process, the most general class of models for forecasting a time series which can be transformed to be “stationary” by differenciation, log transformation or by taking the residuals of a linear regression.

### Stationarising:

The next step is to transform the data to make it stationary.

The evolution of the retail sales data seems to follow a linear trend, let's model it and remove the trend to the
data.

We model the trend using a linear regression.
```{r}
unadjusted.regression <- lm(data$unadjusted~data$adjustments)
sa.unadjusted.regression <- lm(data$seasonally.adjusted~data$adjustments)
```

Then we remove the trend from the data by simply getting the residuals of the regression
```{r}
unadjusted.residuals <- residuals(unadjusted.regression)
sa.residuals <- residuals(sa.unadjusted.regression)
```

We now have the following ACF and PACF:

```{r}
unadjusted.residuals.ts <- ts(unadjusted.residuals, frequency=12)
tsdisplay(unadjusted.residuals.ts)
```

From that plot, we remark that is a seasonal component available with a 12 lag period (see lags 12, 24, 36, etc...)

The goal now is to identify presence of AR and MA components in the residuals.
Because the seasonal pattern is strong and stable, we will want to use an order of seasonal differencing in the model. This, we seasonallly differentiate the residuals with a lat of 12 months.

```{r}
diff.residuals.ts <- diff(unadjusted.residuals.ts, lag=12)
```


Now, let's test that the transformed data trend stationary or if the data requires more levels of differentiation.
```{r}
list(
  adf=adf.test(unadjusted.ts)$p.value < 0.05,
  box=Box.test(unadjusted.ts, lag=12, type='Ljung-Box')$p.value < 0.05,
  kpss=kpss.test(unadjusted.ts)$p.value > 0.05
)
```

The three standard tests for stationarity seem to indicate we don't need to differenciate the data more.


We can then start to anaylise the data in order to estimate a model.
```{r}
tsdisplay(diff.residuals.ts)
```

From the partial autocorrelogram, we see that the partial autocorrelation at `lag 1` is positive and exceeds the significance bounds (~0.5), while the partial autocorrelation at `lag 4` is negative and also exceeds the significance bounds (~-0.3). The partial autocorrelations tail off to zero after lag 4.

Suggest a model blah blah...
The MA(q) process Xt has ρk = 0 for all k, |k| > q. So a diagnostic for MA(q) is
that |rk| drops to near zero beyond some threshold.
Try to fit....

Let's fit the estimated on the transformed data:

```{r}
fit.estimated.model <-
  arima(diff.residuals.ts,
        order=c(1, 0, 4),
        seasonal=list(order=c(0, 1, 1), period=12)
  )

fit.estimated.model
plot(acf(residuals(fit.estimated.model)))
```

We can then try to fit our estimated model on the unadjusted retail sales data:
```{r}
fit.estimated.model.unadjusted.data <-
  arima(unadjusted.ts,
        order=c(1, 0, 4),
        seasonal=list(order=c(1, 1, 0), period=12)
  )

fit.estimated.model.unadjusted.data
```

```{r}
plot(forecast(fit.estimated.model.unadjusted.data))
```

```{r}
par(mfrow=c(2,2))
plot(acf(residuals(fit.estimated.model.unadjusted.data)))
plot(density(residuals(fit.estimated.model.unadjusted.data)))
qqnorm(residuals(fit.estimated.model.unadjusted.data))
qqline(residuals(fit.estimated.model.unadjusted.data), col = 2)
```

```{r, echo=FALSE, message=FALSE}
par(mfrow=c(1,1))
```

Since successive forecast errors seem to be sligthly correlated, and the forecast errors seem to be normally distributed with mean zero and constant variance, the ARIMA(0,1,1) does seem to provide an adequate predictive model.

```{r}
shapiro.test(residuals(fit.estimated.model.unadjusted.data))
```
the p-value is < 0.05 so the test rejects the null hypothesis that the data are normal.
The fact that ACF/PACF plots show corrlation in the errors and their distribution is not normal shows that
the model is not performing very well and thus can be improved upon.

Another way of testing whether there is significant evidence of non-zero correlation is to perform a Ljung-Box test.

```{r}
Box.test(residuals(fit.estimated.model.unadjusted.data), lag=20, type="Ljung-Box")
```

The p-value is 9.396e-06 which is an evidence that of non-zero autocorrelation in the forecast errors.

Manually estimating the parameters of the ARIMA model can be a good approach and give satisfying results, but they are quite hard to identify and extremelly subjective. Using automatic methods should overcome that issue.

Automatic methods search over the space of possible models by minimizing the Akaike’s Information Criterion (AIC) - which measures the goodness of fit for a particular model by balancing the error of the fit against the number of parameters in the model. Other criterions can be used such that the Biases Corrected AIC or the Bayesian Information Criterion.

In the next section, we are going to use a variation of the cross validation adapted to the context of time series forecasting. We will compare different modelling approaches, ARIMA, auto.arima and exponential smoothing (they extrapolate nonseasonal patterns and trends can be extrapolated using a smoothing model).

### Cross Validation:

forecast evaluation with a rolling origin

# accurary,
Now, we want to know how well our model forecast different forecast horizons.
```{r, warning=FALSE}

k <- 12
n <- length(unadjusted.ts)

frequency <- tsp(unadjusted.ts)[3]
start <- tsp(unadjusted.ts)[1] + (k-2)/12
end <- tsp(unadjusted.ts)[2] - k/12

mae.manual.arima <- matrix(NA,ceiling(end-start)*12,12)
mae.auto.arima <- matrix(NA,ceiling(end-start)*12,12)
mae.ets <- matrix(NA,ceiling(end-start)*12,12)

count <- 0
for(i in seq(start, end, by=1/12)) {
  train <- window(unadjusted.ts, end=i)
  test <- window(unadjusted.ts, start=i+1/12, end=i+k/12)

  # manual model:
#    fit.manual.arima <- 
#      arima(train,
#          order=c(1, 0, 4),
#          seasonal=list(order=c(0, 1, 1), period=12)
#    )
#    forecast.manual.arima <- forecast(fit.manual.arima, h=12)
#     
  # auto.arima:  
  fit.auto.arima <- auto.arima(train)
  forecast.auto.arima <- forecast(fit.auto.arima, h=12)

  # ETS
  fit.ets <- ets(train)
  forecast.ets <- forecast(fit.ets, h=12)
  
  # mae.manual.arima[i,1:length(test)] <- abs(forecast.manual.arima[['mean']]-test)
  mae.auto.arima[count, 1:length(test)] <- abs(forecast.auto.arima[['mean']]-test)
  mae.ets[count, 1:length(test)] <- abs(forecast.ets[['mean']]-test)
  count <- count + 1
} 

```


# model 

```{r}
#ARIMA(2,1,0)(1,0,0)[12]
plot(forecast(auto.arima(unadjusted.ts, xreg=fourier(unadjusted.ts, K=5)),xreg=fourierf(unadjusted.ts, K=5, h=100)))


```



