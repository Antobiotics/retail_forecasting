---
title: "CanadianRetailSales"
output: html_document
---

In this document we develop a simple forecasting model of Canadian Retail Sales.
The first thing I done was to look at the data you provided. It describes unadjusted and seasonally adjusted Monthly Retail Trades in Canada from **January 2000** to **July 2015**.

Looking at the date the data were accessed the 20th of October 2015. Looking at the Statistics Canada website, I first noticed that the data were last modified the 22nd of October 2015 and that the numbers were slightly different.

Definition: Retail trade is defined in the International Standard Industrial Classification (ISIC) as the re-sale (sale without transformation) of new and used goods to the general public, for personal or household consumption or utilisation.

By researching the [Statistics Canada help page](http://www23.statcan.gc.ca/imdb/p2SV.pl?Function=getSurvey&SDDS=2406), we note that that seasonally adjusted data is built using the X-12-ARIMA method.

```{r, echo=FALSE, message=FALSE}
# Setup and load required packages.
if(!require("pacman")) {
  install.packages("pacman")
}

pacman::p_load(plyr, dplyr, xts, zoo, ggplot2, forecast, tseries)
```
## Preliminaries:

### Formatting the data:

First the data provided is not ideal for an analysis using R so I had to reformat it.

```{r}
base.data <- read.csv('./data/cansim__Jan-2000_Jul-2015.csv', header=FALSE)

data <-
  base.data %>%
  select(-V1, -V2, -V3) %>% # Remove the description columns
  slice(5:7) %>%            # Only select row containing data
  t %>% data.frame %>%      # Transpose and cast to data.frame
  rename(adjustments=X1) %>%
  rename(unadjusted=X2) %>%
  rename(seasonally.adjusted=X3) %>%
  mutate(seasonally.adjusted=as.numeric(as.character(seasonally.adjusted))) %>%
  mutate(unadjusted=as.numeric(as.character(unadjusted))) %>%
  mutate(adjustments=as.Date(
                        as.character(x=paste("01-", adjustments, sep="")),
                        format="%d-%b-%Y")
         )# Converts the adjustments. You need to paste a day to be able to convert such format to a date.
```



Now it will be easier to work with such format.

### First look at the data:

```{r}
g.data.with.regression <-
  ggplot(data, aes(x=adjustments, y=unadjusted, color='unadjusted')) +
  geom_smooth(method='lm', color='black') + geom_line() +
  geom_line(data=data, aes(x=adjustments, y=seasonally.adjusted, color='seasonally.adjusted')) +
  xlab('Time') + ylab('Retail Sales')

g.data.with.regression
```

From here, we can note a few observations:

* The retail sales are strongly seasonal.
* The time plot shows a sudden change, particulary in 2009.
* The variability of the seasonal components seems constant.

the fluctuations around this trend
have the same intensity

Additive model?

We want to build a model that can forecast unadjusted retail sales data, let's create timeseries objects for
that data.

```{r}
unadjusted.ts <- ts(data$unadjusted, frequency=12)
```

### Stationarity:
From those observations, we can suppose that our time series is not stationary process.
But let's test that assumption using the **Augmented Dickey-Fuller Test**, the **Box test** and the **KPSS test**.


```{r  message=FALSE, warning=FALSE}
results.unadjusted <- list(
  adf=adf.test(unadjusted.ts)$p.value < 0.05,
  box=Box.test(unadjusted.ts, lag=12, type='Ljung-Box')$p.value < 0.05,
  kpss=kpss.test(unadjusted.ts)$p.value > 0.05
)
results.unadjusted
```

From those test, we note that the three tests don't give the same results of stationarity.
Moreover, if we have a look at the Autocorrelation function (ACF) and the Partial autocorrelation function (PACF) of our data. The ACF shows large autocorrelations that diminish very slowly at large lags. This is usually the
signature of a non-stationary time series.

```{r}
tsdisplay(unadjusted.ts, lag=48)
```

Note that the seasonal lags of the PACF and ACF show:
 * exponential decay in the seasonal lags of the ACF
 * a single significant spike at lag 12 in the PACF.

Thus, we can safely assume the data is not stationary which means we need to transform the data such that 
our process is stationary on mean and variance.

Since non stationary a good idea would be to model the data using an ARIMA process, the most general class of models for forecasting a time series which can be transformed to be “stationary” by differenciation, log transformation or by taking the residuals of a linear regression.

### Transforming the unadjusted retail sales data:

The next step is to transform the data to make it stationary.

The evolution of the retail sales data seems to follow a linear trend, let's model it and remove the trend to the
data.

We model the trend using a linear regression.
```{r}
unadjusted.regression <- lm(data$unadjusted~data$adjustments)
sa.unadjusted.regression <- lm(data$seasonally.adjusted~data$adjustments)
```

Then we remove the trend from the data by simply getting the residuals of the regression
```{r}
unadjusted.residuals <- residuals(unadjusted.regression)
sa.residuals <- residuals(sa.unadjusted.regression)
```

We now have the following ACF and PACF:

```{r}
unadjusted.residuals.ts <- ts(unadjusted.residuals, frequency=12)
tsdisplay(unadjusted.residuals.ts)
```

From that plot, we remark that is a seasonal component available with a 12 lag period (see lags 12, 24, 36, etc...)

The goal now is to identify presence of AR and MA components in the residuals.
Because the seasonal pattern is strong and stable, we will want to use an order of seasonal differencing in the model. This, we seasonallly differentiate the residuals with a lag of 12 months.

```{r}
diff.residuals.ts <- diff(unadjusted.residuals.ts, 1, lag=12)
```


Now, let's test that the transformed data trend stationary or if the data requires more levels of differentiation.
```{r}
list(
  adf=adf.test(unadjusted.ts)$p.value < 0.05,
  box=Box.test(unadjusted.ts, lag=12, type='Ljung-Box')$p.value < 0.05,
  kpss=kpss.test(unadjusted.ts)$p.value > 0.05
)
```

The three standard tests for stationarity seem to indicate we don't need to differenciate the data more.


We can then start to anaylise the data in order to estimate a model.

### Esitmating an ARIMA model:

```{r}
tsdisplay(diff.residuals.ts)
```

From the partial autocorrelogram, we see that the partial autocorrelation at `lag 1` is positive and exceeds the significance bounds (~0.5), while the partial autocorrelation at `lag 4` is negative and also exceeds the significance bounds (~-0.3). The partial autocorrelations tail off to zero after lag 4.
Finally, there are three significant spikes in the PACF suggesting a possible AR(3) term. The pattern in the ACF is not indicative of any simple model.

We can then try to fit our estimated model on the unadjusted retail sales data:
```{r}
fit.estimated.model <-
  arima(unadjusted.ts,
        order=c(3, 0, 4),
        seasonal=list(order=c(2, 1, 2), period=12)
  )

fit.estimated.model
plot(acf(residuals(fit.estimated.model)))
```
Let's test whether there is significant evidence of non-zero correlation with a Ljung-Box test.

```{r}
Box.test(residuals(fit.estimated.model), lag=20, type="Ljung-Box")
```

The p-value is 0.2292. There is no evidence of non-zero autocorrelation in the forecast errors.

```{r}
plot(forecast(fit.estimated.model))
```

```{r}
par(mfrow=c(2,2))
plot(acf(residuals(fit.estimated.model)))
plot(density(residuals(fit.estimated.model)))
qqnorm(residuals(fit.estimated.model))
qqline(residuals(fit.estimated.model), col = 2)
```

```{r, echo=FALSE, message=FALSE}
par(mfrow=c(1,1))
```

The successive forecast errors doesn't seem to be  auto-correlated.  The forecast errors seem to be normally distributed but has notable deviations from the straight line are signature of a light left tailed distribution (the density plot confirms this intuition)

```{r}
shapiro.test(residuals(fit.estimated.model))
```
the p-value is < 0.05 so the test rejects the null hypothesis that the data are normal.
The fact that ACF/PACF plots does not show correlation in the errors and their distribution is not normal shows that the model is not performing very well and thus can be improved upon.


Manually estimating the parameters of the ARIMA model can be a good approach and give satisfying results, but they are quite hard to identify and extremelly subjective. Using automatic methods should overcome that issue.

Automatic methods search over the space of possible models by minimizing the Akaike’s Information Criterion (AIC) - which measures the goodness of fit for a particular model by balancing the error of the fit against the number of parameters in the model. Other criterions can be used such that the Biases Corrected AIC or the Bayesian Information Criterion.

In the next section, we are going to use a variation of the cross validation adapted to the context of time series forecasting. We will compare different modelling approaches, ARIMA, auto.arima and exponential smoothing (they extrapolate nonseasonal patterns and trends can be extrapolated using a smoothing model).

### Cross Validation:

forecast evaluation with a rolling origin

# accurary,
Now, we want to know how well our model forecast different forecast horizons.
Mean absolute percentage error
```{r, warning=FALSE}

mape <- function(actual, forecast) {
  return(abs((actual-forecast)/actual))
}

k <- 12
n <- length(unadjusted.ts)

frequency <- tsp(unadjusted.ts)[3]
start <- tsp(unadjusted.ts)[1] + (k-2)/12
end <- tsp(unadjusted.ts)[2] - k/12

default.matrix <- matrix(NA,ceiling(end-start)*12,12)

mape.auto.arima <- default.matrix
mape.ets <- default.matrix
mape.holt.winters <- default.matrix

count <- 0
start <- 3
for(i in seq(start, end, by=1/12)) {
  train <- window(unadjusted.ts, end=i)
  test <- window(unadjusted.ts, start=i+1/12, end=i+k/12)
     
  # auto.arima:  
  fit.auto.arima <- auto.arima(train)
  forecast.auto.arima <- forecast(fit.auto.arima, h=12)

  # ETS
  fit.ets <- ets(train)
  forecast.ets <- forecast(fit.ets, h=12)
  
  fit.holt.winters <- HoltWinters(train, seasonal='additive')
  forecast.holt.winters <- forecast(fit.holt.winters, h=12)
  
  mape.auto.arima[count, 1:length(test)] <- mape(test, forecast.auto.arima[['mean']])
  mape.ets[count, 1:length(test)] <- mape(test, forecast.ets[['mean']])
  mape.holt.winters[count, 1:length(test)] <- mape(test, forecast.holt.winters[['mean']])
  count <- count + 1
} 

mape.horizon <- data.frame(
  index=seq(1, dim(default.matrix)[2]),
  auto.arima=colMeans(mape.auto.arima, na.rm = TRUE),
  ets=colMeans(mape.ets, na.rm = TRUE),
  holt.winters=colMeans(mape.holt.winters, na.rm = TRUE)
)

ggplot(mape.horizon, aes(x=index)) +
  geom_line(aes(y=auto.arima, color='auto.arima')) +
  geom_line(aes(y=ets, color='ets')) +
  geom_line(aes(y=holt.winters, color='holt.winters'))

```


# model 

```{r}
#ARIMA(2,1,0)(1,0,0)[12]
plot(forecast(auto.arima(unadjusted.ts, xreg=fourier(unadjusted.ts, K=5)),xreg=fourierf(unadjusted.ts, K=5, h=100)))


```



